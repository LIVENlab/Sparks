"""
@author: Alexander de TomÃ¡s (ICTA-UAB)
        -LexPascal
"""
import pandas as pd
pd.options.mode.chained_assignment = None  # default='warn'
from Sparks.generic.generic_dataclass import *

bd.projects.set_current(bw_project)



class Cleaner:
    """Clean the input data and modify the units"""

    def __init__(self,
                 motherfile: str,
                 file_handler: dict,
                 national: Optional[bool]= False,
                 ):

        self.national = national
        self.mother_file = motherfile
        self.final_df = None
        self.techs_region_not_included = []
        self.file_handler = file_handler

        self._edited = False


    @staticmethod
    def create_template_df() -> pd.DataFrame:
        """
        Basic template for clean data
        """
        columns = ['spores',"techs","carriers","unit","energy_value"]
        return pd.DataFrame(columns=columns)


    def _load_data(self, source:str) -> pd.DataFrame:
        """ Assuming comma separated input, load the data from a specific file"""
        try:
            return pd.read_csv(self.file_handler[source], delimiter=',').dropna()
        except FileNotFoundError as e:
            raise FileNotFoundError(f"File {source} does not exist") from e


    def _input_checker(self, data: pd.DataFrame, filename: str):
        """
        Check whether the input follows the expected strucutre.
        Assume that last column corresponds to filename (flow_out_sum etc)
        """

        filename = filename.split('.')[0]
        expected_cols = {'spores',
                         'techs',
                         'locs',
                         'carriers',
                         'unit',
                         filename}
        # Add 'spores' column if it does not exist
        if 'spores' not in data.columns:
            data['spores'] = 0

        try:
            tr = data[list(expected_cols)]
            data.rename(columns={filename: 'energy_value'}, inplace=True)
            data['filename'] = filename

            return data


        except KeyError:
            raise KeyError(f"Columns {data.columns} do not match the expected columns: {expected_cols}")


    def _filter_techs(self,df: pd.DataFrame, filter: str)-> pd.DataFrame:
        """
        Filter the input data based on technologies defined in the basefile
        """
        df_names=df.copy()
        # Filter Processors
        df_names['alias_carrier'] = df_names['techs'] + '_' + df_names['carriers']
        df_names['alias_filename'] = df_names['alias_carrier']+ '__' + df_names['filename']
        df_names['full_name'] = df_names['alias_filename'] + '___' + df_names['locs']
        df_names = self._manage_regions(df_names)
        df_names['alias_filename']=df_names['alias_filename']+'___' + df_names['countries']


        if self._edited is False:
            self.basefile = pd.read_excel(self.mother_file, sheet_name='Processors').dropna(subset=['Ecoinvent_key_code'])
            self.basefile['alias_carrier'] = self.basefile['Processor'] + '_' + self.basefile['@SimulationCarrier']
            #self.basefile['alias_region'] = self.basefile['alias_carrier']+'_'+self.basefile['Region']
            self.basefile['alias_filename'] = self.basefile['alias_carrier']+'__'+self.basefile['File_source']
            self.basefile['alias_filename'] = self.basefile['alias_filename'].str.split('.').str[0]
            #TODO: redundant to get it trhough
            self.basefile['alias_filename_loc'] = self.basefile['alias_filename'] + '___' + self.basefile['Region']
            self._edited = True


        basefile = self.basefile.loc[self.basefile['File_source'] == filter]
        excluded_techs = set(df_names['alias_carrier']) - set(basefile['alias_carrier'])
        self.techs_region_not_included=excluded_techs

        df_names = df_names[~df_names['alias_carrier'].isin(excluded_techs)] # exclude the technologies

        return df_names


    def _group_data(self,df: pd.DataFrame)-> pd.DataFrame:
        """
        Group the input data based on technologies defined in the basefile
        If national= True, it aggregates by country
        """

        if self.national:
            pass
            df['locs'] = df['locs'].str.split('_').str[0].str.split('-').str[0]
            grouped_df = df.groupby(['alias_filename', "locs", 'unit'], as_index=False).agg({
                'energy_value': 'sum',
                'spores': 'first',
                'techs': 'first',
                'carriers': 'first',
                'unit': 'first',
            })

        else:
            pass
            grouped_df = df.groupby(['spores',
                                     'techs',
                                     'carriers',
                                     'unit',
                                     'locs',
                                     'alias_carrier',
                                     'alias_filename',
                                     'full_name'
                                     ], as_index=False).agg({
                'energy_value': 'sum'
            })
        return grouped_df


    @staticmethod
    def _manage_regions(df:pd.DataFrame)-> pd.DataFrame:
        """
        Edit the regions format strings in order to get general country names
        """
        df['countries'] = [x.split('_')[0].split('-')[0]
                     for x in df['locs']]
        return df


    def preprocess_data(self)->pd.DataFrame:
        """Run data preprocessing steps"""

        self.basefile = pd.read_excel(self.mother_file, sheet_name='Processors')
        all_data = self.create_template_df()
        grouped = self.basefile.groupby('File_source')

        for data_source, group in grouped:
            pass #TODO: ADD A COLUMN WITH THE FILE NAME
            if pd.isna(data_source):
                warnings.warn(f"DataSource is missing for some entries. Skipping these entries.", Warning)
                continue
            try:
                # Load data once for the data source
                raw_data = self._load_data(data_source)
                checked_data = self._input_checker(data=raw_data, filename = data_source)
                filtered_data = self._filter_techs(checked_data, data_source)
                all_data = pd.concat([all_data, filtered_data], ignore_index=True)

            except Exception as e:
                warnings.warn(f"Error processing {data_source}: {e}", Warning)

        if len(self.techs_region_not_included) > 1:
            formatted_items = "\n".join(f"    - {item}" for item in self.techs_region_not_included)
            message = f"""\nThe following technologies are present in the energy data but not in the Basefile: 
            {formatted_items}

            Please, check the following items to avoid missing information."""
            warnings.warn(message, Warning)

        self.final_df = self._group_data(all_data)
        self.final_df = self._manage_regions(self.final_df)

        return self.final_df


    def _extract_data(self) -> List['BaseFileActivity']:
        base_activities = []

        for _, r in self.basefile.iterrows():
            try:
                base_activities.append(
                    BaseFileActivity(
                        name=r['Processor'],
                        carrier=r['@SimulationCarrier'],
                        parent=r['ParentProcessor'],
                        region=r['Region'],
                        code=r['Ecoinvent_key_code'],
                        factor=r['@SimulationToEcoinventFactor'],
                        full_alias=r['alias_filename_loc']
                    )
                )
            except:
                continue

        return [activity for activity in base_activities if activity.unit is not None]


    def _adapt_units(self):
        """adapt the units (flow_out_sum * conversion factor)"""

        self.base_activities = self._extract_data()
        alias_to_factor = {x.full_alias: x.factor for x in self.base_activities}
        unit_to_factor = {x.full_alias: x.unit for x in self.base_activities}

        self.final_df['new_vals'] = self.final_df['alias_filename'].map(alias_to_factor) * self.final_df['energy_value']
        self.final_df['new_units'] = self.final_df['alias_filename'].map(unit_to_factor)
        return self._final_dataframe(self.final_df)


    def _final_dataframe(self, df):

        cols = ['spores',
                'locs',
                'techs',
                'full_name',
                'carriers',
                'new_units',
                'new_vals']

        df.dropna(axis=0, inplace=True)
        pass
        if self.national:
            df.rename(columns={'alias_filename' : 'full_name'}, inplace=True)
            
        df = df[cols]
        df.rename(columns={'spores': 'scenarios', 'new_vals': 'energy_value'}, inplace=True)
        df['aliases'] = df['techs'] + '__' + df['carriers'] + '___' + df['locs']
        self._techs_sublocations = df['full_name'].unique().tolist()  # save sublocation aliases for hierarchy

        return df


    def adapt_units(self):
        """Public method to adapt the units"""
        return self._adapt_units()












































